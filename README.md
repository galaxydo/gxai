# gx402
so the way it should work that on home page we have first of all container with auth status showing our assistant connected to telegram account or otherwise its button to connect it by adding phone number / verification password; then second container is list of contacts, those which have agents associated to them, and button to hide some of them and can see which agents are connected to this contact (can filter out not to show those without agents connected), and stats next to each. next container is agents, can see how many contacts are using that agent, how many jobs it has processed etc, and can go to page of particular agent to see its code and description, and have a button to add new agent by publishing his code, or next to each agent to add contact for this agent. and then next container, is jobs. jobs are initiated by spawning agent's code to process particular message of contact associated with that agent.

each agent has its own satidb defined schema and each contact also has for its messages, so when job is running it can read those messages history of entire conversation in chat, and inside of its code it can invoke multiple gxai inference steps (to be renamed from Agent) to find out what message to send in response and then it will call our api callback to actually send that message. 

where it gets really powerful that code of agent can be written such as its job (triggered by admin's contact message) it can manage our assistant's agent similarly as user would do through interface uploading agent's code and attaching to agent, but it can do it by itself even writing code of new agent and attaching it to contact, and then sending message, for example:

admin contact sends a message like "schedule a meeting with my friend alex" and admin contact is associated with this powerful agent which can see which other agents exists and can see there is no such agent for scheduling meetings or knowing calendar so it will generate code for such an agent which in its code will check messages history and see what to respond in order to achieve goal through negotiation to schedule meeting at appropriate time so this father agent it creates new meetings agent and then it attaches contact of alex to it and sends message to alex, next time alex sends message to our assistant it will trigger job to be spawned from meetings agent which in turn will check calendar and check all conversation history to reply offering appropriate time slot closest to desire of correspondent.

[![npm version](https://badge.fury.io/js/gx402.svg)](https://badge.fury.io/js/gx402)
[![Bun](https://img.shields.io/badge/Bun-tested-blueviolet)](https://bun.sh/)
[![TypeScript](https://img.shields.io/badge/TypeScript-strict-blue)](https://www.typescriptlang.org/)

gx402 is a lightweight TypeScript library for building AI agents powered by large language models (LLMs). It emphasizes structured inputs and outputs using [Zod](https://zod.dev/) schemas, with built-in support for real-time streaming updates. Perfect for applications needing progressive, token-by-token responses while maintaining type safety and parseable results.

Key features:
- **Structured I/O**: Define input and output schemas with Zod for validation and type inference.
- **Streaming Support**: Receive token-by-token updates for fields, including progressive content building.
- **Agent Abstraction**: Simple `Agent` class to orchestrate LLM calls with customizable temperature and models.
- **Progress Tracking**: Optional callbacks for monitoring stages like streaming or completion.
- **Nested Schema Flattening**: Nested output fields are automatically flattened for streaming (e.g., `analysis.step1` becomes `analysis_step1`).

Supports modern runtimes like Bun and Node.js.

## Installation

Install via npm:

```bash
npm install gx402
```

You'll also need:
- [Zod](https://www.npmjs.com/package/zod) for schemas (`npm install zod`).
- An LLM provider (e.g., OpenAI-compatible API). Set your API key in environment variables (e.g., `OPENAI_API_KEY`).

For testing, use Bun's test runner: `bun add -d bun:test`.

## Quick Start

Create an agent with input/output schemas and run it with a streaming callback.

```typescript
import { z } from 'zod';
import { Agent } from 'gx402';

const agent = new Agent({
  llm: 'o4-mini-2025-04-16', // Your LLM model (e.g., OpenAI GPT variant)
  inputFormat: z.object({
    question: z.string(),
  }),
  outputFormat: z.object({
    analysis: z.object({
      step1: z.string(),
      step2: z.string(),
      step3: z.string(),
    }).describe('Step-by-step analysis of the question'),
    answer: z.string().describe('The final short concise answer to the question'),
    status: z.string().describe('just literal word OK'),
  }),
  temperature: 0.7, // Optional: Controls creativity (0-1)
});

const input = { question: 'What are the benefits of renewable energy?' };

const result = await agent.run(input, (update) => {
  if (update.stage === 'streaming') {
    console.log(`Field: ${update.field}, Partial Value: ${update.value}`);
    // Value builds progressively (e.g., "Renewable energy reduces carbon..." → "...emissions and costs.")
  }
});

console.log('Final Analysis:', result.analysis);
console.log('Final Answer:', result.answer);
console.log('Status:', result.status); // e.g., "OK"
```

### Expected Output
- **Streaming Logs** (real-time, token-by-token):
  ```
  Field: analysis, Partial Value: {"step1":"Renewable energy...
  Field: analysis, Partial Value: {"step1":"Renewable energy sources like solar and wind...
  Field: answer, Partial Value: Renewable energy offers environmental, economic...
  ...
  ```
- **Final Result** (fully parsed and validated):
  ```json
  {
    "analysis": {
      "step1": "Renewable energy sources like solar and wind reduce reliance on fossil fuels.",
      "step2": "They lower greenhouse gas emissions and combat climate change.",
      "step3": "Economically, they create jobs and reduce long-term energy costs."
    },
    "answer": "Renewable energy provides environmental protection, cost savings, and energy independence.",
    "status": "OK"
  }
  ```

Nested fields like `analysis.step1` stream as `analysis` (full JSON object building progressively) or flattened (`analysis_step1`) based on config—check your schema descriptions for hints.

## API Reference

### Agent Constructor
```typescript
new Agent(config: AgentConfig)
```

**AgentConfig**:
- `llm: string` (required): Model identifier (e.g., `'gpt-4o-mini'`, `'o4-mini-2025-04-16'`).
- `inputFormat: ZodObject` (required): Schema for validating inputs.
- `outputFormat: ZodObject` (required): Schema for parsing LLM responses. Use `.describe()` for field hints.
- `temperature?: number` (default: 0.5): Sampling temperature.
- `stream?: boolean` (default: true): Enable streaming.

### agent.run(input: InputType, callback?: UpdateCallback): Promise<OutputType>
- `input`: Object matching `inputFormat`.
- `callback?: (update: ProgressUpdate | StreamingUpdate) => void`: Optional hook for real-time events.
  - **ProgressUpdate**: `{ stage: 'starting' | 'completing' | 'error', message: string }`.
  - **StreamingUpdate**: `{ stage: 'streaming', field: string, value: string }` – `value` grows token-by-token.
- Returns: Parsed output matching `outputFormat`.

### Types
- `ProgressUpdate`: Non-streaming milestones (e.g., "Request sent").
- `StreamingUpdate`: Field-specific partial values (e.g., building JSON for `analysis`).

## Examples

### Basic Non-Streaming
```typescript
const result = await agent.run(input); // No callback, blocks until complete
```

### Custom Error Handling
```typescript
try {
  const result = await agent.run(input, (update) => {
    // Handle updates...
  });
} catch (error) {
  console.error('Agent failed:', error.message); // e.g., "Invalid API key"
}
```

### Nested Streaming
For deeply nested schemas, fields stream as flat strings (e.g., `analysis_step1: "Renewables reduce..."`). Use Zod's `.describe()` to guide the LLM on formatting.

## Testing
gx402 is tested with Bun. Run tests:

```bash
bun test
```

Example test (from `tests/streaming.test.ts`):
```typescript
import { test, expect } from 'bun:test';
import { z } from 'zod';
import { Agent, ProgressUpdate, StreamingUpdate } from 'gx402';

test('should emit streaming progress updates token-by-token', async () => {
  // ... (see full test in repo)
  expect(streamingUpdates.length).toBeGreaterThan(0);
  // Verifies progressive building and final match
}, { timeout: 60000 });
```

Note: Tests skip if API keys aren't set (e.g., for CI).

## Configuration
- **Environment Vars**: `OPENAI_API_KEY` (or provider-specific). Customize via `process.env`.
- **Timeouts**: Set via test options or extend `Agent` for production.
- **Providers**: Defaults to OpenAI-compatible; extend `LLM` class for others (e.g., Anthropic, Grok).

## Contributing
1. Fork and clone.
2. Install deps: `bun install`.
3. Run tests: `bun test`.
4. Lint: `bun run lint`.
5. Submit PRs to `main`.

Issues? Open a ticket on GitHub.

## License
MIT. See [LICENSE](LICENSE). 

Built with ❤️ for structured AI workflows. Questions? Ping `@galaxydoxyz` on X.
